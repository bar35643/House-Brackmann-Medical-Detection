\chapter{Material und Methoden}\label{mat_and_method}
\section{Material}\label{material}
Als Stichprobe stehen die Datensätze von insgesamt 86 Patient*innen zur Verfügung. Bereitgestellt worden sind die Datensätze an Patient*innen vom \textbf{Universitätsklinikum Regensburg}. Die dazugehörigen Grade der House-Brackmann Skala, sind hierbei inderhalb der Stichprobe allerdings nicht gleichverteilt vorhanden. Der Grad VI ist prozentual ungefähr im gleichen Maße vorhanden wie die Grade I-V zusammen (siehe Abb. \ref{cap:pie_grade}). Der Grad I ist nur mit eine*r Patient*in vertreten. Diese ungleiche Verweilung wird bei der späteren Auswirkung berücksichtigt werden.

\begin{figure}[!b]\centering
\begin{tikzpicture}
\pie[sum=auto, radius=2.45, text=pin]{1/I , 6/II, 9/III, 22/IV, 8/V, 40/VI }
%\pie[pos ={10,0}, sum=auto, radius=3.5, text=legend]{1/I , 6/II, 9/III, 22/IV, 8/V, 40/VI }
\end{tikzpicture}
\caption[Verteilung der einzelnen Grade der House-Brackmann Skala]{Verteilung der einzelnen Grade der House-Brackmann Skala des zu Verfügung stehenden Datensatzes. Klar erkennbar ist das ungleiche Vorhandensein der Grade.}\label{cap:pie_grade}
\end{figure}\label{fig:pie_grade}


 Jeder Datensatz jede*r Patient*in besteht aus 9 verschiedenen Bilder, die verschiedenste Posen darstellen. Die Gesichtsausdrücke in den Einzelbilder können jeweils einen konkreten Teil des House-Brackmann Scores zugeordnet werden. Zudem werden die normalerweise in Bewegung stattfindenden Ausdrücke statisch im Bild eingefangen. Die Bildcodierung (Posendarstellung) dieser 9 Bilder lautet:

\begin{enumerate}
  \setlength\itemsep{-0.6em}
\item Ruhender Gesichtsausdruck
\item Augenbrauen heben
\item Lächeln, geschlossener Mund
\item Lächeln, geöffneter Mund
\item Lippen schürzen, \glqq Duckface\grqq{}
\item Augenschluss, leicht
\item Augenschluss, forciert
\item Nase rümpfen
\item Depression Unterlippe
\end{enumerate}

Das Bild mit der Codierung 1 fokussiert sich dabei auf die Symmetrie in Ruhe. Die zweite Pose (2) bildet die Stirn ab. Dadurch sollen die Auswirkungen und die Beweglichkeit der Muskeln rund um die Stirnpartie bildlich festgehalten werden (absichtliche Faltenbildung). Der Lidschluss wird mit den Bildern 6 und 7 statisch eingefangen. Ein nicht geschlossenes Auge wird durch das Weiße im Augenball für das System erkennbar. Die Gesichtsausdrücke auf den Bildern 3, 4, 5, 8 und 9 zeigen den Mund in unterschiedlichsten Positionen. So sollen die Unterschiede zwischen den beiden Gesichtshälften am besten aufgezeigt werden. Die Posen für den Mund sind von außen betrachtet einer der Hauptmerkmale für eine Fazialisparese, welche auch schon für einen Laien erkennbar ist.

Problematisch dabei ist, dass nicht für jede*n Patient*in alle 9 korrespondierenden Bilder vorhanden sind. Besonders ist dies bei den Bildcodierungen 5, 7, 8 und 9 der Fall (siehe Abb. \ref{cap:bar_code}). Beim Trainieren der Neuronalen Netze muss Rücksicht darauf genommen werden.


\begin{figure}[!tb]\centering
\begin{tikzpicture}
\begin{axis}
[
    ybar,
    ymin=0,
    %enlargelimits=0.15,
    ylabel={\#Anzahl}, % the ylabel must precede a # symbol.
    xlabel={\ Bildcodieung},
    symbolic x coords={Soll, 1, 2, 3, 4, 5, 6, 7, 8, 9},
    xtick=data,
    nodes near coords, % this command is used to mention the y-axis points on the top of the particular bar.
    nodes near coords align={vertical},
    ]
\addplot coordinates {(Soll,86) (1,84) (2,79) (3,79) (4,75)
                      (5,62)    (6,80) (7,68) (8,64) (9,60) };

\end{axis}
\end{tikzpicture}
\caption[Anzahl der vorhandenen  Einzelbilder aller Patient*innen]{Anzahl der vorhandenen  Einzelbilder aller Patient*innen. Der Sollwert beträgt 86 für alle Bildcodierungen.}\label{cap:bar_code}
\end{figure}\label{fig:bar_code}





\clearpage
\section{Methode}\label{method}
\subsection{Module}\label{module} %Modulverarbeitung
Für die Detektion des Grades nach House-Brackmann, werden die einzelnen Merkmale der Skala in Module eingeteilt (siehe Abb. \ref{cap:module_graph}). Ziel dieser Aufteilung ist es, Expertensysteme für die Teilbereiche der House-Brackmann Skala zu bilden. Diese muss für die Detektion angepasst werden. Die dynamischen Eigenschaften Stirn, Lidschluss und Mund können so nicht als ein Label interpretiert werden. Die 9 Eingabebilder in das System stellen die Posen statisch dar. So können alle Eigenschaften bis auf Lidschluss nach der House-Brackmann Skala auf die Grundlabel \glqq Normal\grqq{}, \glqq minimale Asymmetrie\grqq{}, \glqq Asymmetrie\grqq{} und \glqq Keine\grqq{} aufgeteilt werden. Bei Lidschluss besitzt nur noch die zwei Möglichkeiten, \glqq vollständig geschlossen\grqq{} oder \glqq geöffnet\grqq{} zu sein (siehe Tabelle \ref{cap:housebrackmannnew}).
Nach dieser Tabelle werden so für die einzelnen Module die passenden Label bestimmt. Die Spalten sind dabei die einzelnen Module. Die Überschrift der Spalte bezeichnet das einzelne Modul. Der Inhalt dieser sind die zu ermittelnden Klassen für das Modul.

\begin{table}[h]\vspace{1ex}\centering
  \begin{tabular}{c||c|c|c|c|}
  Grad & Symmetrie    & Stirn  & Lidschluss    & Mund         \\ \hline\hline
  I    & Normal       & Normal & Vollständig   & Normal       \\ \hline
  II  & Normal & Normal                                                        & Vollständig & \begin{tabular}[c]{@{}c@{}}minimale\\ Asymmetrie\end{tabular} \\ \hline
  III & Normal & \begin{tabular}[c]{@{}c@{}}minimale\\ Asymmetrie\end{tabular} & Vollständig & \begin{tabular}[c]{@{}c@{}}minimale\\ Asymmetrie\end{tabular} \\ \hline
  IV   & Normal       & Keine  & Unvollständig & Asymmetrisch \\ \hline
  V    & Asymmetrisch & Keine  & Unvollständig & Asymmetrisch \\ \hline
  VI   & Keine        & Keine  & Unvollständig & Keine        \\ \hline
  \end{tabular}
  \caption[Angepasste House-Brackmann Tabelle zur Bestimmung der Label/Klassen]{Angepasste House-Brackmann Tabelle zur Bestimmung der Label/Klassen aus der Originaltabelle \ref{cap:housebrackmann} (eigene entwicklete Interpretation)}\label{cap:housebrackmannnew}
\vspace{2ex}\end{table}\label{table:housebrackmannnew}



\begin{figure}[!tb]\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,semithick]
  \tikzstyle{every state}=[fill=cyan,draw=black,text=black]

  \tikzstyle{level1} = [rectangle, draw, fill=green!40!blue!20, text width=8cm, text centered, inner sep=1pt  , minimum height=1.3cm]
  \tikzstyle{level2} = [rectangle, draw, fill=blue!20,          text width=2.4cm, text centered, rounded corners, minimum height=4cm]
  \tikzstyle{level3} = [rectangle, draw, fill=green!40!blue!20, text width=8cm, text centered, inner sep=1pt  , minimum height=1.3cm]
  \tikzstyle{level4} = [rectangle, draw=white, fill=white, text width=1cm, text centered, minimum height=0.1cm, minimum width=0.1cm]

  \tikzstyle{container} = [draw, rectangle, dashed, inner sep=8pt, minimum width=15cm]

  \node[level4]         (Z1)                                 {};

  \node[level1]         (A) [left of=Z1, xshift=8cm]        {Regions of Interest};
  \node[level2]         (B) [below left  of=A, yshift=-2cm] {Lidschluss \\$y_l = [vollständig,\linebreak unvollständig]$};
  \node[level2]         (C) [below right of=A, yshift=-2cm] {Mund \\$y_m = [normal,\linebreak minasymm,\linebreak asymm,\linebreak keine]$};
  \node[level2]         (D) [left  of=B, xshift=-1cm]       {Symmetrie \\$y_s = [normal,\linebreak asymm,\linebreak keine]$};
  \node[level2]         (E) [right of=C, xshift=1cm]        {Stirn \\$y_f = [normal,\linebreak minasymm,\linebreak keine]$};
  \node[level3]         (F) [below right of=B, yshift=-2cm] {Fusionierung};
  \node[container, fit=(B) (C) (D) (E)] (his) {};
    \node at (his.north west) [above right,node distance=0 and 0] {Module};

  \node[level4]         (Z2) [right of=F, xshift=3cm]        {};

  \path (Z1) edge [out=0   , in=180] node [left, xshift=-0.3cm, align=right]                {Input \\ Bilder $x_1 ... x_9$} (A)

        (A) edge [out=-145, in=90 ] node [right, yshift=-0.1cm, xshift=0cm   ] {$x_{l}$} (B) %Lidschluss
            edge [out=-35 , in=90 ] node [left , yshift=-0.1cm, xshift=0cm   ] {$x_{m}$} (C) %Mund
            edge [out=-160, in=90 ] node [above, yshift=-0.5cm, xshift=0.8cm ] {$x_{s}$} (D) %Symmetrie
            edge [out=-20 , in=90 ] node [above, yshift=-0.5cm, xshift=-0.5cm] {$x_{f}$} (E) %Stirn
        (B) edge [out=-90 , in=145] node [right, yshift=0.1cm , xshift=0cm   ] {$y_{l}$} (F)
        (C) edge [out=-90 , in=35 ] node [left , yshift=0.1cm , xshift=0cm   ] {$y_{m}$} (F)
        (D) edge [out=-90 , in=160] node [below, yshift=0.5cm , xshift=0.8cm ] {$y_{s}$} (F)
        (E) edge [out=-90 , in=20 ] node [below, yshift=0.5cm , xshift=-0.5cm] {$y_{f}$} (F)

        (F) edge [out=0   , in=180] node [right, xshift=0.3cm, align=left]                {Output \\ H-B Grad $[I, ..., VI]$} (Z2);

\end{tikzpicture}
\caption[Darstellung der Modulbauweise]{{Schematische Darstellung der Module. Die Eingabebilder $x_1, ..., x_9$ werden für jedes Bild in die Regions of Interest zerschnitten. Diese wandern als Stack $x_s, x_l, x_m, x_f$ (zerschnittenes Bildmaterial) in die einzelnen vorgesehenen Module. Danach werden diese zum \ac{hb_grade} fusioniert.}\label{cap:module_graph}}
\end{figure}\label{fig:module_graph}

Anhand von Markerpunkten im Gesicht der Patient*innen sollen die 9 Bilder jede*r Patient*in in die Bestandteile der Skala, die Regions of Interest, zerschnitten. Dabei werden nur die relevanten Teile betrachtet, um proaktiv zu verhindern, dass die Neuronalen Netze nicht notwendige Eigenschaften als relevante Merkmale interpretieren. Zu den nicht notwendigen Bestandteilen der Bilder zählen Hintergrund, welches verschiedenste Farben und Helligkeitsstufen annehmen kann und der Körper, ab dem Kinn abwärts der Patient*innen. Nur der Kopf wird für die Feststellung der Parese benötigt. Die Landmarks, die für die Einteilung in die Regionen verantwortlich sind, müssen hierfür immer an derselben Position $P$ im generierten Array sein (siehe Abb. \ref{cap:r_of_interest}). Jedem Modul können so die relevanten Punkte (siehe Tabelle \ref{cap:rel_landmarks}) für ihr Modul zugeordnet werden und das Ausgabearray, welches die 9 Bildausschnitte beinhaltet. Für die Berechnung der neuen Eckpunke für den Bildausschnitt werden die zugeordneten Landmarks $P$ in die nachfolgenden Formeln eingesetzt \cite{s151026756}:

\noindent\begin{minipage}{.5\linewidth}
\begin{alignat*}{3}
  a_{min} &= min(P[:, 0]) &\quad &\textrm{| Extrema links}\\
  a_{max} &= max(P[:, 0]) &\quad &\textrm{| Extrema rechts}\\
  b_{min} &= min(P[:, 1]) &\quad &\textrm{| Extrema oben}\\
  b_{max} &= max(P[:, 1]) &\quad &\textrm{| Extrema unten}
\end{alignat*}
\end{minipage}%
\begin{minipage}{.5\linewidth}
  \begin{equation}
  \begin{split}
    &\textrm{Zu erfüllende Bedingungen:}\\
    &\textrm{1.}\quad  a_{min} < a_{max}\\
    &\textrm{2.}\quad  b_{min} < b_{max}\\
    &\textrm{3.}\quad a_{min}, a_{max}, b_{min}, b_{max} \in \mathbb{N}
  \end{split}
  \label{eg:bbox}
  \end{equation}
\end{minipage}

\vspace{0.3cm}

\begin{table}[b]\vspace{1ex}\centering
\begin{tabular}{c||c|c|}
Modul & \begin{tabular}[c]{@{}c@{}}zugeschnittene\\ Eingabebilder als\\ Stack\end{tabular} & \begin{tabular}[c]{@{}c@{}}Betrachtete\\ Landmarks\end{tabular} \\ \hline\hline
Symmetrie  & $x_s$  & P(00) - P(68)  \\ %\hline
Lidschluss & $x_l$  & P(38) - P(47) \\ %\hline
Mund       & $x_m$  & P(48) - P(68) \\ %\hline
Stirn      & $x_f$  & P(17) - P(27) \\ \hline
\end{tabular}
\caption[Zuordnung der Module zu dem relevanten Punkten der Landmarks und das Eingabebild in die Module nach dem Ausschneiden vom Originalbild $x$]{Zuordnung der Module zu den relevanten Punkten der Landmarks und die Eingabebilder in die Module nach dem Ausschneiden vom Originalbild $x$}\label{cap:rel_landmarks}
\vspace{1ex}\end{table}\label{fig:rel_landmarks}


Dabei sind ($a_{min}, b_{min}$), ($a_{min}, b_{max}$), ($a_{max}, b_{min}$) und ($a_{max}, b_{max}$) die vier neuen Ecken des benötigten Bildfragments. Die Ecken spannen ein Rechteck auf, dessen Inhalt das Eingabebild für eines der Module (z.B. Lidschluss nur Punkte $P(36)$ bis $P(47)$) relevant ist. Alles was sich nicht innerhalb des Rechteckes befindet, wird weggeschnitten. Ein Offset wird dazu noch in alle Richtungen addiert bzw. subtrahiert, um Ungleichheiten in der Landmarkberechnung auszugleichen, falls die Kontur nicht korrekt getroffen wurde. Wenn 9 Bildfragmente für eines der Module berechnet worden sind, werden diese als Stack in die nächsttiefere Schicht weitertransportiert.

\begin{figure}[!t]\centering
%\resizebox{1.2\textwidth}{!}{
  %width=0.98\textwidth
\vspace{-1cm}
\makebox[0pt]{\includesvg[inkscapelatex=false, width=30cm]{./images/landmarks}}
%}
\vspace{-1cm}
\caption[Anschauliche Darstellung der Regions of Interest]{Anschauliche Darstellung der Regions of Interest\footnotemark. Inhald des roten Rahmens ist die Eingabe für das Modul Symmetrie, grün für die Stirn, gelb der Lidschluss und pink für den Mund. (Aus Datenschutzgründen durch einen Sobel-Filter verändert!)}\label{cap:r_of_interest}
\end{figure}\label{fig:r_of_interest}


Die einzelnen Module Symmetrie, Lidschluss, Mund und Stirn (siehe Abb. \ref{cap:module_graph}) sind vortrainierte ResNet18 Netze vom ImageNet Contest. Benutzen von vortrainierten Netzen spart Trainingszeit an den Layern durch die schon vorhandenen angepassent Parametern vom ImageNet Contest. Die einzelnen Gewichte an den Kanten müssen so nur noch verfeinert werden, um damit das gewünschte Ergebnis zu erzielen. Für alle Experimente (siehe Kapitel \ref{experiment}) ist das gewählte Netz identisch, damit die Ergebnisse vergleichbar sind. Die Abbildungsvorschrift für die Relation Eingabebild $x$ mit der Pixelgröße $a, b$ und drei Schichten für den Farbraum RGB (bei Konkatenation vielfaches von 3) zu auszugebenden Wahrscheinlichkeiten der Klassen $y$ mit der Klassenlänge $k$ ist:

\begin{equation}
y: \mathbb{R}^{3 \times a \times b} \to \mathbb{R}^{k}, y(x) = resnet18(x)
\label{eq:resnet}
\end{equation}

Jedem Eingabebild ist eine natürliche Zahl zugeordnet, die innerhalb der Trainingsphase und der Evaluierungsphase zu erimitteln ist. Diese Zahl ist letztendlich eine Nummer, die ein Label/Klasse nach der oben genannten Tabelle repräsentiert. Nummeriert werden diese von 0 beginnend. Für das Modul Symmetire und Stirn 0-2, Lidschluss 0 und 1, Mund 0-3. Die Operation \ref{eq:resnet} liefert Wahrscheinlichkeiten für alle Klassen eines Moduls. Das Ziel ist diese Zahl zu treffen. Wird die Nummer vom Modell getroffen, ist das somit eine korrekte Klassifizierung. Innerhalb der Trainingsphase werden diese Modelle anhand des Losses, die Abweichung zwischen den ermittelten aus dem Modul und der realen Klasse, das Neuronale Netz pro Modul optimiert.

Im Anschluss, nachdem alle Klassen der separaten Module ausgerechnet wurden, können diese fusioniert werden, um den Grad nach House-Brackmann zu bestimmen. Dabei gibt es zwei Vorgehensweisen, die methodisch zu bestätigen sind (siehe Kapitel \ref{fusion}), dass sie den Grad Ordnungsgemäß ermitteln. Dabei können einmal alle Wahrscheinlichkeiten direkt als Zeilensumme gebildet werden. So kann direkt, der am höchsten befindlichen Wert der Zeilensumme der Tabelleneinträge, den an der linken Seite befindlichen Grad für eine*n Patient*in angenommen werden. Eine andere Herangehensweise ist das hinzuziehen eines Automaten. Dazu werden zuerst die Wahrscheinlichkeiten in die jeweils den Modulen zugeordneten Label umgemünzt. Die Position des höchsten Wertes im zurückgelieferten Array des Ausgabewertes der Neuronalen Netze ist gleich der Position des Labels nach der Tabellenspalte von der angepassten House-Brackmann Tabelle. Danach durchlaufen alle 4 finalen Klassen jedes Moduls den Automaten. Der Endzustand des Automaten repräsentiert den zu ermittelnden Grad nach House-Brackmann.


\subsection{Direkte Gradermittlung}\label{di_module}
\begin{figure}[b]\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,semithick]
  \tikzstyle{every state}=[fill=cyan,draw=black,text=black]

  \tikzstyle{level1} = [rectangle, draw, fill=green!40!blue!20, text width=8cm, text centered, inner sep=1pt  , minimum height=1.3cm]
  \tikzstyle{level2} = [rectangle, draw, fill=blue!20,          text width=2.4cm, text centered, rounded corners, minimum height=4cm]
  \tikzstyle{level3} = [rectangle, draw, fill=green!40!blue!20, text width=8cm, text centered, inner sep=1pt  , minimum height=1.3cm]
  \tikzstyle{level4} = [rectangle, draw=white, fill=white, text width=1cm, text centered, minimum height=0.1cm, minimum width=0.1cm]

  \tikzstyle{container} = [draw, rectangle, dashed, inner sep=8pt, minimum width=15cm]

  \node[level4]         (Z1)                          {};

  \node[level1]         (A) [left of=Z1, xshift=8cm]  {Regions of Interest};
  \node[level3]         (F) [below of=A]              {ResNet18 Klassifizierer};

  \node[level4]         (Z2) [right of=F, xshift=3cm] {};

  \path (Z1)edge [out=0  , in=180] node [left, xshift=-0.3cm, align=right]{Input \\ Bilder $x_1 ... x_9$} (A)
        (A) edge [out=270, in=90 ] node [right, yshift=-0.1cm, xshift=0cm] {$x_{s}$} (F)
        (F) edge [out=0  , in=180] node [right, xshift=0.3cm, align=left]  {Output \\ H-B Grad $[I, ..., VI]$} (Z2);

\end{tikzpicture}
\caption[Direkte Ermittlung des \ac{hb_grade}]{Direkte Ermittlung des \ac{hb_grade}. Die Bildfragmente $x_s$ die als Eingabe in den Klassifizierer dienen sind dieselben wie bei der Modulform (siehe \ref{module}) für die Symmetrie.}\label{cap:di_graph}
\end{figure}\label{fig:di_graph}

Zum Vergleich wird auch noch der House-Brackmann Skala der Grad der Fazialisparese auf dem direkten Weg, ohne die Skala in Module zu zerlegen, ermittelt. So soll festgestellt werden, ob die Zerlegung in Module sinnvoll ist und inwiefern die Ermittlungsgenauigkeit und Präzision sich unterscheiden. Auch soll so eine Richtung erkennbar sein ob sich überhaupt aus den 9 gegebenen Bildern pro Patient*in der Grad ermitteln lässt. Der Aufbau unterscheidet sich minimal von der Modulbauweise. Dabei wird derselbe Stack an Eingabebebildern ($x_s$) von dem Modul für die Symmetrie genutzt. Diese stellt, wie bekannt, das komplette Gesicht der/die Patient*innen dar. Einzig allein die Klassifikationslabel sind unterschiedlich. Anstelle der Klassen für das Modul der Symmetrie $[normal, asymm, keine]$, werden die Label $[I, II, III, IV, V, VI]$ der House-Brackmann Skala als die zu ermittelnde Klassen verwendet (siehe Abb. \ref{cap:di_graph}). Eingespart werden alle Berechnungen zu den anderen Modulen und die im Nachhinein stattfindende Fusionierung der Ergebnisse zur Einordnung des Grades nach House-Brackmann.




\subsection{Oversampling zum Klassenausgleich}\label{oversamplingmethod}
\begin{figure}[!b]\centering
    \begin{minipage}{0.5\textwidth}
        %\centering
        \hspace*{-0.7cm}
        \begin{tikzpicture}
        \pie[sum=auto, radius=2, text=pin]{38/normal , 8/asymm, 40/keine} %Symm
        \end{tikzpicture}
        \caption*{\textbf{(a)} Klassenverteilung für Modul Symmetrie}\label{cap:pie_a}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        %\centering
        \vspace*{1.1cm}
        \hspace*{-0.9cm}
        \begin{tikzpicture}
        \pie[sum=auto, radius=2, text=pin]{70/vollständig , 16/unvollständig}  %Lidschluss
        \end{tikzpicture}
        \vspace*{0.4cm}
        \caption*{\textbf{(b)} Klassenverteilung für Modul Lidschluss}\label{cap:pie_b}
    \end{minipage}

    \vspace{0.5cm}


    \begin{minipage}{.5\linewidth}
      \begin{tikzpicture}
      \pie[sum=auto, radius=2, text=pin]{7/normal , 9/minasymm, 70/keine} %Stirn
      \end{tikzpicture}
      \vspace*{1cm}
      \caption*{\textbf{(c)} Klassenverteilung für Modul Stirn}\label{cap:pie_c}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \begin{tikzpicture}
      \pie[sum=auto, radius=2, text=pin]{1/normal , 15/minasymm, 30/asymm, 40/keine} %Mund
      \end{tikzpicture}
      \caption*{\textbf{(d)} Klassenverteilung für Modul Mund}\label{cap:pie_d}
    \end{minipage}
    \caption[Veranschaulichung der ungleichen Klassenverteilung jedes Modules]{Veranschaulichung der ungleichen Klassenverteilung jedes Modules (a-d) anhand des gegebenen Datensatzes. Bei einer Gleichverteilung müssen alle Teile gleich groß sein.}\label{cap:vert_pie}
\end{figure}\label{fig:vert_pie}


Die gegebenen Datensätze von Patient*innen mit ihren House-Brackmann Graden ist unausgewogen (siehe Kapitel \ref{material}). Somit sind auch die Klassen für jedes der Module nicht gleichverteilt (siehe Abb. \ref{cap:vert_pie}). Um eine Ausgewogenheit herzustellen, wird das Oversampling angewentet. Hierzu werden die Gewichte $\omega$ für jedes Label aus einem Modul ausgerechnet. Allgemein ist Oversampling eine Methode zum Ausgleich von nicht in gleichen Teilen vorhandenen Klassen. Es ist letztenendes eine Überabtastung eines gegebenen Datensatzes. Dazu wird die Anzahl der Klasse $l$ in den Term (\ref{eq:weight}) hineingegeben. Die einzelnen Gewichte müssen dabei addiert nicht $100\%$ ergeben.

\begin{equation}
\omega = 1 / l, \qquad l \in \mathbb{N} \quad \land \quad l \neq 0 \label{eq:weight}
\end{equation}

Ziel ist es einen ausgewogenen Datensatz zu generieren. Je höher die Anzahl einer Klasse ist, desto niedriger wird das Gewicht $\omega$. Ist eine Klasse eines spezifischen Modules niedrig, so ist ihr Gewicht dagegen hoch. Durch geschicktes Auswählen aus dem Datensatz, durch die zuvor berechneten Gewichte jeder Klasse, kann so eine gleiche Verteilung dieser erzeugt werden. Dies erfolgt durch das Duplizieren der mit höheren gewichteten Klassen. Problematisch ist jedoch, dass Bilder von Patient*innen so mehrfach vorkommen. Das kann sich negativ auf die Trainingsphase der Neuronalen Netze auswirken. Diese können durch die Mehrfachbenutzung, falsche Rückschlüsse zur Klassifikationsermittlung ziehen und so das Leistungsvermögen mindern. Oversampling wird ausschließlich auf den Trainingsdatensatz bezogen. Der Evaluierungsdatensatz bleibt unangetastet und wird nicht ausgeglichen.

Im Folgenden werden verschiedenste Experimente mit unterschiedlichsten Vorgehensweisen der Datensatzpräparation die Aussage überprüfen, ob Oversampling einen Vorteil oder ein Nachteil ist.



\clearpage
\section{Vorgehensweise}\label{process}
In diesem Kapitel werden drei Vorgehensweisen für die Zusammensetzung der 9 verschiedenen Bilder jede*r Patient*in, die während der Trainings- und Evaluierungsphase zum Einsatz kommt, erläutert. Zur Vereinfachung wird dies nur für die Modulform dargestellt. Analog gilt, dass diese Anwendungsformen auch für die direkte Gradermittlung (siehe Kapitel \ref{di_module}) genutzt wird. Für die direkte Ermittlung des Grades nach House-Brakckmann fällt der erste Schleifendurchgang, für die einzelnen Module Symmetrie, Lidschluss, Stirn und Mund, in den nachfolgenden Sequenzdiagrammen weg, da nur ein Neuronales Netz benötigt wird das direkt den Grad feststellt.

\subsection{Sequenzielles Verfahren}\label{sequential_method}
\begin{figure}[!b]\centering
\makebox[0pt]{\includesvg[inkscapelatex=false, width=1.0\textwidth]{./images/Sequenziell}}
\caption[Sequenzdiagramm des Ablaufprozesses bei der sequenziellen Bearbeitung der 9 Bilder]{Sequenzdiagramm des Ablaufprozesses bei der sequenziellen Bearbeitung der 9 Bilder. Diese durchlaufen hintereinander dasselbe Neuronale Netz.}\label{cap:seq}
\end{figure}\label{fig:seq}

Die Anordnung der Bilder während der Trainings- und Evaluierungschritte ist hierbei sequenziell. Das bedeutet im Trainingsprozess, dass jedes zugeschnittene Bild der 9 Posen jede*r Patient*in für ein Modul hintereinander das Neuronale Netz durchlaufen, bevor ein neuer Zyklus (Epoch) beginnt. Im dargestellten Sequenzdiagramm (siehe Abb. \ref{cap:seq}) ist zu sehen, dass nach der Wahrscheinlichkeitsermittlung für die Klassifikation des einzelnen Modules die Rückwärtsrechnung anhand des Losses das Neuronale Netz optimiert wird. Eines der Probleme, die auftauchen können, ist, dass zum Beispiel der Trainingsfortschritt, der durch das erste Bild erzielt wurde, direkt vom nächsten wieder zerstört werden kann. Vorstellbar ist, dass die Posen, die in jedem Bild unterschiedlich dargestellt werden, eine komplementäre Wirkung auf das Modell erzielt.

Währenddessen im Evaluierungsprozess wird die beste Wahrscheinlichkeit der 9 Durchgänge als das der wahren Klassifikation angesehen. So ist vermeidbar, dass die schlechteren Klassifikationswahrscheinlichkeiten eine zu starke negative Auswirkung haben.

Falls von den 9 Bildposen der/die Patient*in einzelne Posen fehlen und so während des Evaluierungs- und Trainingsprozesses nicht vorhanden sind, werden diese nicht beachtet. Der Schleifendurchlauf wird in einem solchen Fall abgebrochen, sodass die nicht vorhandene Pose keine Auswirkungen auf die Neuronalen Netze der Module haben.


\subsection{Early Fusion}\label{earlyfusion_method}
Eine andere Möglichkeit zur Eingabe in das Modell ist die Konkatenation der 9 Bilder vorab. Die Hintereinanderreihung der 3 Farbschichten RGB von den vorab zugeschnittenen Bildmaterialen ($x_1$ bis $x_9$) für ein spezifisches Modul erzeugt so einen Tensor mit der Größe $[27, a, b]$ mit $a, b$ als Pixelgröße (siehe Abb. \ref{cap:ef_layer}). Die Architektur der Neuronalen Netze sind dabei jedoch identisch zu dem sequenziellen Vorgehen. Danach werden diese in das Modell zur Klassifikation der Klassen des jeweiligen gerade bearbeiteten Modules. Ziel ist es, das Neuronale Netz selbst entscheiden zu lassen, welche Regionen in den Bildern von Relevanz zur Bestimmung des Grades nach House-Brackmann sind.

Die nicht vorhandenen Bildcodierungen werden in dem Fall Schwarz (Tensor mit nur den Werten 0) dargestellt bzw. generiert. So soll verhindert werden, dass sich die Parameter an den Kanten der Knoten in den Neuronalen Netz verändern und der Fortschritt zunichte gemacht wird.


\begin{figure}[!b]\centering
  \resizebox{0.95\textwidth}{!}{%
\begin{tikzpicture}[->,>=stealth',shorten >=3pt,auto,node distance=0.3cm,semithick]
  \tikzstyle{every state}=[fill=cyan,draw=black,text=black]

  \tikzstyle{red}   = [rectangle, draw=gray, fill=red!100  , text width=1cm, text centered, inner sep=1pt  , minimum height=1cm, minimum width=1cm, line width=2pt]
  \tikzstyle{green} = [rectangle, draw=gray, fill=green!100, text width=1cm, text centered, inner sep=1pt  , minimum height=1cm, minimum width=1cm, line width=2pt]
  \tikzstyle{blue}  = [rectangle, draw=gray, fill=blue!100 , text width=1cm, text centered, inner sep=1pt  , minimum height=1cm, minimum width=1cm, line width=2pt]
  \tikzstyle{black} = [rectangle, draw=gray, fill=black    , text width=1cm, text centered, inner sep=1pt  , minimum height=1cm, minimum width=1cm, line width=2pt]
  \tikzstyle{white} = [rectangle, draw=white, fill=white    , text width=0.3cm, text centered, inner sep=1pt  , minimum height=0.3cm, minimum width=1cm, line width=2pt]

  \node[blue]    (A1) [xshift=8cm]                {};
  \node[green]   (B1) [left of=A1, yshift=0.15cm] {};
  \node[red]     (C1) [left of=B1, yshift=0.15cm] {};
  \node[black]   (A2) [left of=C1, yshift=0.15cm] {};
  \node[black]   (B2) [left of=A2, yshift=0.15cm] {};
  \node[black]   (C2) [left of=B2, yshift=0.15cm] {};
  \node[blue]    (A3) [left of=C2, yshift=0.15cm] {};
  \node[green]   (B3) [left of=A3, yshift=0.15cm] {};
  \node[red]     (C3) [left of=B3, yshift=0.15cm] {};

  \node[blue]    (A4) [left of=C3, yshift=0.15cm] {};
  \node[green]   (B4) [left of=A4, yshift=0.15cm] {};
  \node[red]     (C4) [left of=B4, yshift=0.15cm] {};
  \node[blue]    (A5) [left of=C4, yshift=0.15cm] {};
  \node[green]   (B5) [left of=A5, yshift=0.15cm] {}; %mitte
  \node[red]     (C5) [left of=B5, yshift=0.15cm] {};
  \node[blue]    (A6) [left of=C5, yshift=0.15cm] {};
  \node[green]   (B6) [left of=A6, yshift=0.15cm] {};
  \node[red]     (C6) [left of=B6, yshift=0.15cm] {};

  \node[black]   (A7) [left of=C6, yshift=0.15cm] {};
  \node[black]   (B7) [left of=A7, yshift=0.15cm] {};
  \node[black]   (C7) [left of=B7, yshift=0.15cm] {};
  \node[blue]    (A8) [left of=C7, yshift=0.15cm] {};
  \node[green]   (B8) [left of=A8, yshift=0.15cm] {};
  \node[red]     (C8) [left of=B8, yshift=0.15cm] {};
  \node[blue]    (A9) [left of=C8, yshift=0.15cm] {};
  \node[green]   (B9) [left of=A9, yshift=0.15cm] {};
  \node[red]     (C9) [left of=B9, yshift=0.15cm] {};

  \tikzstyle{level1} = [rectangle, draw, fill=green!40!blue!20, text width=4cm, text centered, inner sep=1pt  , minimum height=1.3cm]
  \tikzstyle{level2} = [rectangle, draw, fill=green!40!blue!20, text width=2.5cm, text centered, inner sep=1pt  , minimum height=1.3cm]

  \node[level1]     (Model) [right of=B8, xshift=8cm] {Model des jeweiligen Modules};
  \node[level2]     (aaa) [right of=B4, xshift=-10cm] {Dataloader};
  \node[white]     (spacer) [left of=Model,xshift=-6cm] {};


  \node[red]     (C9) [left of=B9, yshift=0.15cm] {};

  \path (spacer)edge [out=0  , in=180] node {$Tensor[27, a, b]$} (Model)
  (aaa)edge [] node [xshift=1cm, yshift=0.5cm] {$x_1, ..., x_9 \in Tensor[3, a, b]$} (B9)
  (aaa)edge [] node {} (B8)
  (aaa)edge [] node {} (B7)
  (aaa)edge [] node {} (B6)
  (aaa)edge [] node {} (B5)
  (aaa)edge [] node {} (B4)
  (aaa)edge [] node {} (B3)
  (aaa)edge [] node {} (B2)
  (aaa)edge [] node {} (B1);

\end{tikzpicture}
}%
\caption[Darstellung der Konkatenation der 9 zugeschnittenen Bilder eines Modules in Schichtform]{Darstellung der Konkatenation der 9 zugeschinttenen Bilder eines Modules in Schichtform. Die Schwarzen Schichten sind die aus den Bildern entstandenen Tensoren gefüllt nur mit 0 Werten. Ausgabetensor nach der Operation wird dann in das Neuronale Netz für das jeweilige Modul gegeben.}\label{cap:ef_layer}
\end{figure}\label{fig:ef_layer}

\begin{figure}[!t]\centering
\makebox[0pt]{\includesvg[inkscapelatex=false, width=1.0\textwidth]{./images/Early Fusion}}
\caption[Sequenzdiagram des Ablaufprozesses bei Benutzung von Early Fusion]{Sequenzdiagram des Ablaufprozesses bei Benutzung von Early Fusion. Dabei werden die 9 zugeschnittenen Bilder für jedes Modul vorab konkateniert.}\label{cap:early}
\end{figure}\label{fig:early}

\vspace{5cm}

\subsection{Late Fusion}\label{latefusion_method}

\begin{figure}[b]\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,semithick]
  \tikzstyle{every state}=[fill=cyan,draw=black,text=black]

  \tikzstyle{level1} = [rectangle, draw, fill=green!40!blue!20, text width=8cm, text centered, inner sep=1pt  , minimum height=1.8cm]
  \tikzstyle{level4} = [rectangle, draw=white, fill=white, text width=1cm, text centered, minimum height=0.1cm, minimum width=0.1cm]

  \tikzstyle{level2} = [rectangle, draw, fill=green!40, text width=0.1cm, text centered, inner sep=1pt  , minimum height=1.2cm, minimum width=0.6cm]

  \node[level4] (Z1)                          {};

  \node[level1] (A) [below of=Z1, yshift=0.5cm]  {};
  \node[level2] (B1) [below of=Z1, yshift=0.5cm, xshift=-3.6cm]  {$x_1$ \\ $\downarrow$ \\ $y_1$};
  \node[level2] (B2) [right of=B1, xshift=-1.6cm]  {$x_2$ \\ $\downarrow$ \\ $y_2$};
  \node[level2] (B3) [right of=B2, xshift=-1.6cm]  {$x_3$ \\ $\downarrow$ \\ $y_3$};
  \node[level2] (B4) [right of=B3, xshift=-1.6cm]  {$x_4$ \\ $\downarrow$ \\ $y_4$};
  \node[level2] (B5) [right of=B4, xshift=-1.6cm]  {$x_5$ \\ $\downarrow$ \\ $y_5$};
  \node[level2] (B6) [right of=B5, xshift=-1.6cm]  {$x_6$ \\ $\downarrow$ \\ $y_6$};
  \node[level2] (B7) [right of=B6, xshift=-1.6cm]  {$x_7$ \\ $\downarrow$ \\ $y_7$};
  \node[level2] (B8) [right of=B7, xshift=-1.6cm]  {$x_8$ \\ $\downarrow$ \\ $y_8$};
  \node[level2] (B9) [right of=B8, xshift=-1.6cm]  {$x_9$ \\ $\downarrow$ \\ $y_9$};

  \node[level4]         (Z2) [below of=A, yshift=0.5cm] {};

  \path (Z1)edge [out=-90  , in=90] node [left, xshift=1.3cm, yshift=1cm, align=center]{Input Bilder \\ $x_1 ... x_9$} (A)
        (A) edge [out=-90  , in=90] node [left, xshift=2.0cm, yshift=-1cm, align=center]{Output \\ Fusioniertes Ergebnis} (Z2);
        %(F) edge [out=0  , in=180] node [right, xshift=0.3cm, align=left]  {Output \\ H-B Grad $[I, ..., VI]$} (Z2);

\end{tikzpicture}
\caption[Darstellung eines Modules mit den 9 Neuronalen Netzen]{Darstellung eines Modules mit den 9 Neuronalen Netzen. Der hellblaue Rahmen repräsentiert dabei ein Hauptmodul (Symmetrie, Lidschluss, Stirn, Mund) und die grünen Rechtecke sind dabei die Sub-Netze für die Bilder $x_1$ - $x_9$. Das Ergebnis wird im Anschluss vor der Rückgabe fusioniert.}\label{cap:mod_lf}
\end{figure}\label{fig:mod_lf}


Late Fusion ist das exakte Gegenteil von Early Fusion. Jedes Bild enthält ein eigenes Neuronales Netz und das für alle Module. Das bedeutet, dass insgesamt bei 4 Modulen und 9 Bildcodierungen 36 separate Netze benötigt werden. All diese Netze müssen getrennt voneinander trainiert werden (siehe Abb. \ref{cap:late}). Bei dem Evaluierungspozess werden zuerst alle Modelle, die demselben Modul zugeordnet sind fusioniert. Es gilt, dass die beste Klasse über alle 9 Netze (siehe Abb. \ref{cap:mod_lf}) diejenige ist, die für die spätere Zusammenfügung zu dem House-Brackmann Grad genommen wird. Das resultiert daraus, da angenommen werden kann, dass diese am wahrscheinlichsten die Realität abbildet. Sinn dahinter ist, die Posen als eine Gruppe zu interpretieren, welche zusammengehören. Die Modelle sollen also spezifisch auf die eingegebene Pose in die Sub-Netze reagieren und den Loss minimieren.

Dadurch dass nicht alle Bildcodierungen für alle Patient*innen vorhanden sind, ist es notwendig, diese aus dem Trainingsprozess zu entfernen. Dazu kann, wenn eines der 9 Bilder nicht vorhanden ist, der Schleifendurchgang übersprungen werden.



\begin{figure}[!t]\centering
\makebox[0pt]{\includesvg[inkscapelatex=false, width=1.0\textwidth]{./images/Late Fusion}}
\caption[Sequenzdiagram des Ablaufprozesses bei Benutzung von Late Fusion]{Sequenzdiagram des Ablaufprozesses bei Benutzung von Late Fusion. Hier existiert für jedes Bild ein separates Modell. Für jedes Modus sind so 9 Modelle im Einsatz, woraus insgesamt 36 unabhängige Neuronale Netze entstehen.}\label{cap:late}
\end{figure}\label{fig:late}





\clearpage
\section{Fusionierung der Module}\label{fusion}
Für die Fusionierung der einzelnen Module zu dem zu ermittelnden Grades nach House-Brackmann, werden zwei verschiednene Vorgehensweisen vorgesstellt. Als erstes wird Mithilfe eines Automaten die Feststellung des Grades versucht, anhand besten Klassen jedes Modules (Kapitel \ref{automata}). Die zweite, direktere Möglichkeit, ist das ausrechnen der Zeilensumme mit den direkt ausgegebenen Wahrscheinlichkeiten der einzelnen Klassen von den Neuronalen Netzen der Module (Kapitel \ref{rowsum}).

\subsection{Automat}\label{automata}
%\definecolor{mycolor}{RGB}{50,180,255}
\begin{figure}[b]
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,semithick]
  \tikzstyle{every state}=[fill=cyan,draw=black,text=black]

  \node[initial,state] (A)                    {$q_{I}$};
  \node[state]         (B) [right of=A]       {$q_{II}$};
  \node[state]         (C) [right of=B]       {$q_{III}$};
  \node[state]         (D) [right of=C]       {$q_{IV}$};
  \node[state]         (E) [right of=D]       {$q_{V}$};
  \node[state]         (F) [right of=E]       {$q_{VI}$};

  \path (A) edge                 node {$y_m\geqslant1$} (B)
            edge [bend right=40] node {$y_f\geqslant1$} (C)
            edge [bend right=40] node {$y_e=1$}         (D)
            edge [bend right=40] node {$y_s\geqslant1$} (E)
            edge [bend right=40] node {$y_s\geqslant1$} (E)
        (B) edge [bend left=40]  node {$y_m\geqslant2$} (D)
        (C) edge                 node {$y_f=2$}         (D)
            edge [bend right=40] node {$y_m=2$}         (D)
        (D) edge [bend left=40]  node {$y_m=3$}         (F)
            edge [bend left=40]  node {$y_m=3$}         (F)
        (E) edge                 node {$y_s=2$}         (F)
            edge [bend right=40] node {$y_m=3$}         (F);

\end{tikzpicture}
\caption[Nichtdeterministischer endlicher Automat zur Ermittlung der Schweregrades nach House-Brackmann mit dargestellten Gewichten der Module]{Nichtdeterministischer endlicher Automat zur Ermittlung der Schweregrades nch House-Brackmann mit dargestellten Gewichten der Module}\label{cap:automata}
\end{center}
\end{figure}\label{fig:automata}

Der Grad der Fazialisparese nach House-Brackmann kann durch einen Automaten bestimmt werden. Zuerst werden die Ausgabewahrscheinlichkeiten der einzelnen Module in die wahrscheinlichtste Klasse umgewandelt. Dabei ist der Endzustand des Automaten, der aus den Einzelergebnissen der Modulen resultierenden Klassen,der Grad des House-Brackmann Skala. Das Eingabewort in den Automat sind die Ausgabe der Module. Das Wort dass in den Automaten hineingegeben werden kann setzt sich aus den Bestandteilen $y_s, y_f, y_m, y_e$ zusammen. Die Funktionsnahmen sind hierbei die Wörter und die Ergebnisse sich die Gewichte an den Kanten.

Fix ist das Eingabewort \flqq $y_sy_ey_fy_m$\frqq, der für exakt diesen nichtdeterministischen Automaten (Abb. \ref{cap:automata}) eingegeben werden muss, damit die Reihenfolge gewährleistet ist. Durch den Nichtdeterminismus wird entschieden, wenn keine Änderung des Zustandes mehr möglich ist, dass der nächste Buchstabe an der Reihe ist. So müssen auch nicht alle Transitionsfunktionen zwischen den Kanten und Knoten definiert werden. Leermengen in der Transitionstabelle (Tabelle \ref{cap:transition}) sind die nicht definierten Kanten. Der erste Eingabebuchstabe wird aus der Eingabe genommen (\flqq $y_sy_ey_fy_m$\frqq $\rightarrow$ \flqq $y_ey_fy_m$\frqq). Dieser Prozess wird solange wiederholt, bis das Eingabewort vollständig abgearbeitet ist. Zu Beachten sind dabei die Übergangsfunktionen zwischen zwei Zusänden. Jeder Buchsabe im Eingabewort hat eine Gewichtung (Ergebnis der Funktion), die entscheidet, ob der Pfad passierbar ist (Abb. \ref{cap:automata}). Der Startzustand in den Automaten ist $q_I$, welche den niedrigsten annzunehmenden Grad nach der House-Brakcmann Tabelle darstellen soll. Die Enzustände $q_I$ - $q_{VI}$  sind alle möglichen Grade der Skala.

\begin{table}[t]\vspace{1ex}\centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{c||c|c|c|c|}
  %\backslashbox{Room}{Date}
  \backslashbox{$Q$}{$\Sigma$} & $y_s$ & $y_e$ & $y_m$ & $y_f$
  \\\hline\hline
  $q_I$    & $q_{V}$, wenn $y_s\geqslant1$&$q_{IV}$, wenn $y_e\geqslant1$&$q_{II}$, wenn $y_m\geqslant1$&$q_{III}$, wenn $y_f\geqslant1$\\
  $q_{II}$ & $\varnothing $               &$\varnothing $                &$q_{IV}$, wenn $y_m\geqslant2$&$\varnothing $                 \\
  $q_{III}$& $\varnothing $               &$\varnothing $                &$q_{IV}$, wenn $y_m=2$        &$q_{IV}$, wenn $y_f=2$         \\
  $q_{IV}$ & $\varnothing $               &$\varnothing $                &$q_{VI}$, wenn $y_m=3$        &$\varnothing $                 \\
  $q_{V}$  & $q_{VI}$, wenn $y_s=2$       &$\varnothing $                &$q_{VI}$, wenn $y_m=3$        &$\varnothing $                 \\
  $q_{VI}$ & $\varnothing $               &$\varnothing $                &$\varnothing $                &$\varnothing $
  \\\hline
  \end{tabular}%
  }
  \caption[Übergangsfunktion $\delta$ des Automaten für die Bestimmung des Grades]{Übergangsfunktion $\delta$ des Automaten für die Bestimmung des Grades}\label{cap:transition}
\vspace{2ex}\end{table}\label{table:transition}

Der \ac{nea} für die Bestimmung des Grades nach der Hosue-Brackmann Skala besteht aus dem Fünftupel $N = (Q, \Sigma, \delta, E, F)$:

\begin{itemize}
  \setlength\itemsep{-0.5em}
\item $Q=\{ q_I, q_{II}, q_{III}, q_{IV}, q_{V}, q_{VI} \}$  ist eine endliche Zustandsmenge, wobei der Index representativ der Grad ist, der ermittelt werden soll.
%\item $\Sigma={y_{symmetrie}, y_{lidschluss}, y_{mund}, y_{strin}}$ ist ein endliches Alphabet (in dem Fall speziell die Ausgabe der Funktionen)
\item $\Sigma=\{ y_s, y_f, y_m, y_e \}$ ist ein endliches Alphabet (speziell begrenzt auf die Module)
\item $\delta:Q \times \Sigma \rightarrow P(Q)$ Nicht-Deterministische Übergangsfunktion (siehe Tabelle \ref{cap:transition})
\item $E \subseteq Q = \{ q_I \}$ ist der Startzustand
\item $F \subseteq Q = Q$ Menge aller akzeptierten Endzustände
\end{itemize}





\subsection{Gradermittlung durch Zeilensumme}\label{rowsum}
Eine weitere Moglichkeit ist die Bildung von der Zeilensumme aus den einzelnen vorher definierten Modulen. Dazu werden die Klassenwahrscheinlichkeiten, die das Ergebniss der ausgeführten Prediction der Neuronalen Netze sind, an der Stelle ihres Modules (Reihe der Tabelle) und der Klasse in die abgewandelten House-Brackmann Tabelle eingetragen. Nach dem das für alle Module geschehen ist, werden die Zeilen addiert (Tabelle \ref{cap:rowsum}).

Das höchste Ergebnis der 6 Zeilensummen ist der für den Patient*in angenommene Grad. Die Zeile ohne die einzelwahrscheinlichkeit wird als Ausgageergebnis zurückgeliefert.

\begin{table}[b]\vspace{1ex}\centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{c||cc|cc|cc|cc||c|}
      & \multicolumn{2}{c|}{Symmetrie}     & \multicolumn{2}{c|}{Stirn}           & \multicolumn{2}{c|}{Lidschluss}           & \multicolumn{2}{c||}{Mund}            &      \\ \cline{2-9}
  \multirow{-2}{*}{Grad} &
    \multicolumn{1}{c|}{Klasse} &
    $y_s$ &
    \multicolumn{1}{c|}{Klasse} &
    $y_f$ &
    \multicolumn{1}{c|}{Klasse} &
    $y_l$ &
    \multicolumn{1}{c|}{Klasse} &
    $y_m$ &
    \multirow{-2}{*}{$\Sigma$} \\ \hline\hline
  I   & \multicolumn{1}{c|}{normal} & 0.10 & \multicolumn{1}{c|}{normal}   & 0.02 & \multicolumn{1}{c|}{vollständig}   & 0.30 & \multicolumn{1}{c|}{normal}   & 0.19 & 0.61 \\ %\hline
  II  & \multicolumn{1}{c|}{normal} & 0.20 & \multicolumn{1}{c|}{normal}   & 0.03 & \multicolumn{1}{c|}{vollständig}   & 0.21 & \multicolumn{1}{c|}{minasymm} & 0.18 & 0.62 \\ %\hline
  III & \multicolumn{1}{c|}{normal} & 0.30 & \multicolumn{1}{c|}{minasymm} & 0.04 & \multicolumn{1}{c|}{vollständig}   & 0.12 & \multicolumn{1}{c|}{minasymm} & 0.03 & 0.49 \\ %\hline
  \rowcolor[HTML]{9AFF99}
  IV &
    \multicolumn{1}{c|}{\cellcolor[HTML]{9AFF99}Normal} &
    0.40 &
    \multicolumn{1}{c|}{\cellcolor[HTML]{9AFF99}keine} &
    0.05 &
    \multicolumn{1}{c|}{\cellcolor[HTML]{9AFF99}unvollständig} &
    0.46 &
    \multicolumn{1}{c|}{\cellcolor[HTML]{9AFF99}asymm} &
    0.16 &
    \textbf{1.07} \\ %\hline
  V   & \multicolumn{1}{c|}{asymm}  & 0.50 & \multicolumn{1}{c|}{keine}    & 0.10 & \multicolumn{1}{c|}{unvollständig} & 0.13 & \multicolumn{1}{c|}{asymm}    & 0.15 & 0.88 \\ %\hline
  VI  & \multicolumn{1}{c|}{keine}  & 0.60 & \multicolumn{1}{c|}{keine}    & 0.05 & \multicolumn{1}{c|}{unvollständig} & 0.22 & \multicolumn{1}{c|}{keine}    & 0.14 & 1.01 \\ \hline
  \end{tabular}%
  }
  \caption[Darstellung der Einzelwahrscheinlichkeit aller Modulen und ihren Klassen für die Zeilensummenbildung]{Darstellung der Einzelwahrscheinlichkeit aller Modulen und ihren Klassen für die Zeilensummenbildung. Für dieses Beispiel ist Grad IV der als Resultat ausgegebene Wert.}\label{cap:rowsum}
\vspace{2ex}\end{table}\label{table:rowsum}





\clearpage
\section{Caching}\label{caching}
Problematisch ist, bei größer werdenden Datensätzen, dass die Laufzeit sehr schnell sehr groß werden kann. Dazu muss der Server, der die Trainingsphase der Neuronalen Netze betreut, für alle Epochs, bei allen Bildern die Berechnung der Punkte ausführen, die Bilder in die Regions of Interest zerlegen und die Augmentierung durchführen. Ausserdem muss für die ganzen ResNet18 Netze der Module, die Forwärtsrechnung ausgeführt und Rückwärtsoptimierung anhand des Losses optimiert werden. Die Schritte Punkteberechnung und Einteilung sind für alle Epochs identisch. Die Idee zur Beschleunigung der Trainingsschritte ist, diese fertigen zerschnittenen Bildfragmente in einem Cache vorab abzuspeichern, um diese Schritte einzusparen.

Zwei mögliche Lösungsansätze sind \ac{lru} (Kapitel \ref{lrucache}) und eine stationäre externe Datenbank (Kapitel \ref{database}) als, der als temporären Cache, während der Trainingsphase agiert.

\subsection{Least Recently Used Cache}\label{lrucache}
\ac{lru} bedeutet, der am längsten nicht zugegriffenen Eintrag wird aus dem Cache entfernt. Vergleichen lässt sich das mit einer Warteschlange mit begrenztem Platzinhalt. Aufgerufene Elemente reihen sich am Ende der Warteschlange ein (im dargestellten Array die linke Seite). Alle nachfolgenden wandern um eine Position nach vorne. Der Vorderste in der Warteschlange wird entfernt (siehe Abb. \ref{cap:lrucache}), sobald sich ein noch nicht bekanntes Element in die Warteschlange einreiht. Dieses neue Element muss zur Laufzeit initial berechnet werden. Danach ist es so lange verfügbar, bis es wieder herausgeschoben wurde.

\begin{figure}[b]\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,semithick]

  \tikzstyle{style} = [rectangle, draw, fill=green!40!blue!20, text centered, inner sep=1pt  , minimum height=2cm, align=center]

  \node[style] (A)              {Cache\\ {[A,B,C,D,E]}};
  \node[style] (B) [right of=A] {Cache\\ {[\textbf{B},A,C,D,E]}};
  \node[style] (C) [right of=B] {Cache\\ {[\textbf{E},B,A,C,D]}};
  \node[style] (D) [right of=C] {Cache\\ {[\textbf{Z},E,B,A,C]}};

  \path (A) edge  node [yshift=-1cm, align=center] {B\\ \\ \textcolor{green}{Hit}} (B)
        (B) edge  node [yshift=-1cm, align=center] {E\\ \\ \textcolor{green}{Hit}} (C)
        (C) edge  node [yshift=-1cm, align=center] {Z\\ \\ \textcolor{red}{Miss}}  (D);

\end{tikzpicture}
\caption[Beispiel des \ac{lru}]{Hit sind Treffer. Diese Elemente sind bereits im Cache vorhanden. Miss hingegen sind dem Cache nicht bekannt, diese werden berechnet und reihen sich ein. Beispiel des \ac{lru} mit $l=5$ und dem $\Sigma=A-Z$. Wenn Z sich einreiht, fliegt D raus, da kein Platz mehr vorhanden ist.}\label{cap:lrucache}
\end{figure}\label{fig:lrucache}

Einträge im Cache haben einen Index. Über diesen werden auf die dahinterliegenden Elemente, das Bildmaterial der/die einzelnen Patient*innen, zugegriffen. Sei $l$ die Länge des Caches und $\Sigma$ Indizes (z.B. A-Z), so können insgesamt $l$ Elemente im Cache für das schnelle Aufrufen vorgehalten werden. Alle nicht vorgehaltene Einträge müssen berechnet werden. Die im Cache liegenden Elemente besitzen eine kürzere Aufrufdauer als diejenigen, die nicht im Cache sind. Die \ac{wcet}, also die Zeit, welche benötigt wird, um ein Element aufzurufen, beträgt für

\begin{itemize}
  \setlength\itemsep{-0.5em}
\item im Cache liegend: \qquad\, $WCET_{ges}=WCET_{cache}$
\item nicht im Cache liegend: $WCET_{ges}=WCET_{cache} + WCET_{berechnung}$
\end{itemize}

Dabei ist $WCET_{cache}$ die Laufzeit, die für das Nachschauen der Elemente im Cache benötigt wird und $WCET_{berechnung}$ für die initiale Berechnung des abzuspeichernden Elementes beträgt. Solange $l$ nicht wesentlich kleiner als die Anzahl von allen Elementen im Dataloader ist, können größtenteils Einträge wiederverwertet werden, wenn ein neuer Epoch in der Trainingsphase der Neuronalen Netze beginnt. Die Berechnung der recycelten Elemente kann demzufolge eingespart werden, woraus eine kürzere Gesamtlaufzeit zu erwarten ist.




\subsection{Externe Datenbank}\label{database}
Eine weitere Möglichkeit ist es, Elemente in eine relationale Datenbank zu speichern (siehe Tabelle \ref{cap:database}). Der Aufbau ist dabei relativ identisch zu dem \ac{lru}. Über einen Index wird auf das benötigte Element zugegriffen. Jedoch sind \textbf{alle Elemente} in der Datenbank gespeichert. Die \ac{wcet} beträgt schlussendlich immer die Zeit, welche zum Zugriff auf die Datenbank benötigt wird. Zusätzlich kommt auf die globale Laufzeit einmal die Rechenzeit hinzu, welche benötigt wird, um alle Elemente zu berechnen und in die Datenbank einzutragen. Solange nicht auf den selben Index geschrieben wird, ist es ratsam, die Einträge parallel in die Datenbank zu schreiben und zu berechnen. Das hat den Vorteil, dass das komplette Potential des Servers/Computer ausgenutzt werden kann. Die Indexe repräsentieren dabei eine Nummer, die jeden einzelnen Patient*in zugeordnet ist. Indexe müssen als Primärschlüssel eineindeutig sein, damit keine Konflikte beim Schreiben und Lesen auftreten.

\begin{table}[h]\vspace{1ex}\centering
  \begin{tabular*}{9cm}{c|c}
  \textbf{Indizes (Primärschlüssel)} & \textbf{Elemente}
  \\\hline
  A   &  Bild 1 \\
  B   &  Bild 2 \\
  C   &  Bild 3 \\
  D   &  Bild 4 \\
  E   &  Bild 5 \\
  ... & ...

  \\\hline
  \end{tabular*}
  \caption[Beispieltabelle einer relationalen Datenbank]{Beispieltabelle in einer relationalen Datenbank. Jeder Index besitzt genau ein Element, das ihm zugeordnet ist. Bei einem Zugriff auf den Index wird das Element als Rückgabewert ausgegeben.}\label{cap:database}
\vspace{2ex}\end{table}\label{table:database}


Vorteil dieser Methode ist es, die kurze Lesezeit von der Datenbank ($WCET_{read}$) auszunutzen. Diese ist wesentlich kleiner als die Berechnung der Landmarks und das Einteilen der Regionen zur Laufzeit ($WCET_{berechnung}$). In der Trainingsphase werden $z$ Epochs durchlaufen. Dadurch werden statt  $WCET_{berechnung} * z$ nur $WCET_{read} * z$ Zeiteinheiten benötigt, wobei gilt $WCET_{berechnung} > WCET_{read}$. Dadurch kann die Trainingsphase schneller beendet werden oder auch ermöglicht werden, in der selben Laufzeit, mehr Epochs unterzubringen.
































%\section{?Use Cases}\label{usecase}
% \paragraph{Definition:}
% Use Case Diagramme auch bekannt als „Anwendungsfalldiagramm“ werden verwendet, um das Verhalten des Systems aus Sicht der Benuter zu modellieren.
% Benuter (dargestellt als Strichmännchen) oder Systeme sind Akterue, die miteinander agieren, die den jeweiligen \Acp{uc} abbilden. Die jeweiligen Relationen zwischen Akteure und den \ac{uc} werden mit Linien verbunden. Möglich sind auch Verbingung zwischen \ac{uc}. Dabei können sie andere \ac{uc} optional Erweitern (extend) oder Inkludieren einen weiteren, der den Basis Use Case erweitert. Auch sind Generalisierungen (Vererbung) möglich. So werden alle Eigenschaften und Fälle auf die Spezialisierung übernommen. \textbf{TODO Quelle!!!}
%
%
% \paragraph{Use Cases des Experimentes (siehe Abb. \ref{cap:usecase}):}
% Nach Erfolgrecher Beendigung des Experimentes können die Anwendungsfälle definiert werden. Folgende Akterure kristallisieren sich aus den \ac{uc} heraus:
% \begin{itemize}
% \item Benutzer*innen (Benutzer*innen/Patient*innen, die eine Kategorisierung erwünscht)
% \item Administrator*innen (Verwalter des Systems)
% \item Docker Container (Instanz auf einem Server, der den Sourcecode laufen lässt und über eine Schnittstelle den Zugriff ernöglicht)
% \end{itemize}
%
% \begin{figure}[h]
% \begin{center}
%  \includesvg[width=0.98\textwidth]{./images/UseCase}
% \caption[Use Case Diagramm]{Anwendungsfälle des Experimentes als Use Cases beschrieben.}\label{cap:usecase}
% \end{center}
% \end{figure}\label{fig:usecase}
%
% Administrator*innen verwalten im allgemeinen das System. Diese führen die Trainingsphase aus und versuchen, so genau wie möglich, die Ergebnisse für den Hosue-Brackmann Grad, zu ermitteln. Dabei handelt sich es um ein Optimierungsproblem. Parameter können die Lernrate der Neuronalen Netze beeinfulssen. Auch welches Neuronale Netz zur Anwendung kommt wird von ihnen beeinflusst.
%
% Über einen API Zugriff können so Benutzer*innen für sich selbst die Detection starten indem sie über einen Request an den Server, der die Komponenten hostet, stellt. Dieser sendet ihnen nach der Berechnung das Ergebnis zurück. Im Hintergrund führt der Server, in dem Fall dargestellt als Docker-Container, der über eine IP-Adresse für die Aussenwelt erreichbar ist, die Berechnung aus.
%
%
% \begin{figure}[h]
% \begin{center}
%  \includesvg[width=0.98\textwidth]{./images/API_Request}
% \caption[Aktivitätsdiagramm eines schematischen Ablaufes eines  Requestes]{Aktivitätsdiagramm eines schematischen Ablaufes eines Requestes.}\label{cap:apirequest}
% \end{center}
% \end{figure}\label{fig:apirequest}
